/*
 * Orca Project: Components for robotics 
 *               http://orca-robotics.sf.net/
 * Copyright (c) 2004-2006 Alex Brooks, Alexei Makarenko, Tobias Kaupp
 *
 * This copy of Orca is licensed to you under the terms described in the
 * ORCA_LICENSE file included in this distribution.
 *
 */

/*!

@page orca_doc_about About Orca

@note Reviewed for release 2.0.0.

@section orca_doc_about_toc Contents

- @ref orca_doc_about_purpose
- @ref orca_doc_about_difference
- @ref orca_doc_about_terminology
- @ref orca_doc_about_approach
    - @ref orca_doc_about_whyice
    - @ref orca_doc_about_flexible
    - @ref orca_doc_about_tools
    - @ref orca_doc_about_repository
- @ref orca_doc_about_license
- @ref orca_doc_about_copyright
- @ref orca_doc_about_history

@section orca_doc_about_purpose Purpose

Orca is an open-source framework for developing component-based robotic systems. It provides the means for defining and developing the building-blocks which can be pieced together to form arbitrarily complex robotic systems, from single vehicles to distributed sensor networks. The project's goals are:
- to \b enable software reuse by defining a set commonly-used interfaces;
- to \b simplify software reuse by providing libraries with a high-level convenient API; and
- to \b encourage software reuse by maintaining a repository of components.

@section orca_doc_about_difference What Makes Orca Different?

Orca, currently in its second generation, differs from other robotic software frameworks in that it:
- adopts a Component-Based Software Engineering approach;
- uses  an industrial-strength library (Ice) for communication and interface definition;
- aims to be general, flexible and extensible, in order to place as few constraints as possible on system design;
- provides optional tools to assist in the development of individual components and the management of large systems; and
- maintains a repository of free, reusable components.

@section orca_doc_about_terminology Orca Terminology

When talking about how components connect to each other it's helpful to define some terminology.

A \em platform is a logical unit, a robot would typically be a platform. We use the term platform so not to exclude simple devices like cameras or PDA's. Calling them robots would be a stretch. Platforms are the highest level in our hierarchy, so we don't define 'teams of platforms' for example. Each platform can have one or several \em hosts -- physical computers. Each host may execute one or several \em components. Each component may have zero or more \em interfaces, some are required and some are provided.

These four terms are often used in describing a distributed system but one has to be careful to keep in mind that they refer to different aspects of the system. Platforms describe a logical grouping of hardware and software. This grouping is useful because it reflects physical proximity; cheap/fast/reliable communication, etc. Hosts are units of computing hardware. Components and interfaces describe software organization. Assigning software (components) to hardware (hosts) is called deployment.

@image html orca_terminology.png

In this illustration platform X has one computer host (1) which executes one component (4). Platform Y has two hosts. One would say that Component 3 is deployed on Host 2 which is located on Platform Y. 

Component 1 requires Interface A and it is connected to the provided Interfaces A and B of Component 2. Interface connections can be intra-host (Interface C), intra-platform (Interfaces A and B of Component 2), or inter-platform (Interfaces A and B of Component 3).



@section orca_doc_about_approach Approach

The approach we take is frequently called Component-Based Software Engineering (CBSE). For a broad overview of "componentry" look in <a href="http://en.wikipedia.org/wiki/Software_componentry" target="_blank">Wikipedia</a>. For a summary of CBSE and its application to robotics, read <a href="http://orca-robotics.sf.net/documents/brooks05_towards_component_based_robotics.pdf">a paper by Orca developers</a>.  While it was written for Orca1, the arguments for CBSE apply equally to Orca2.

@subsection orca_doc_about_whyice Why Use Ice?

In order to implement a distributed component-based system, one needs
some kind of <a href="http://en.wikipedia.org/wiki/Middleware" target="_blank">middleware</a> package
to enable inter-component communication.  If support for C/C++ on
Linux is a requirement (which discounts Microsoft's COM+ and Sun's
Enterprise JavaBeans), the following options exist today:

- using XML-based technologies such as SOAP,
- using CORBA,
- writing custom middleware from scratch, or
- using <a href="http://www.zeroc.com" target="_blank">Ice</a>.

We discount XML-based technologies on the grounds that they are too
inefficient for low-level robotic control tasks.

While CORBA is sufficiently flexible for Orca's middleware
requirements, it is also large and complex.  Experience with CORBA in
Orca1 showed this complexity to be problematic.  Ice is far less
complex: for a details see <a href="http://www.zeroc.com/iceVsCorba.html" target="_blank">Ice vs. Corba comparison</a> or read <a href="www.triodia.com/staff/michi/ieee/ieee.pdf" target="_blank">a paper by ZeroC stuff</a>.

Orca1 also experimented with writing custom middleware from scratch.
While communicating over a socket is simple, implementing middleware
sufficiently flexible and reliable to support Orca's requirements
involved re-implementing (and maintaining) large parts of CORBA
functionality, which is a non-trivial task.  It is unrealistic to
expect robotics researchers to have the time or skills to develop
middleware to the same standards as experienced middleware
professionals such as the developers of Ice.

We have been very pleased with Ice so far: it's easy enough to
understand that we managed to get a system working fairly quickly, and
we've become confident in its reliability.

@subsection orca_doc_about_flexible General, Flexible and Extensible

Orca aims to be as broadly applicable as possible by making as few
assumptions as possible.  In particular, we make @b no @b assumptions about:

- @b Architecture: system developers are free to compose a system from any set of components, arranged to form arbitrary architectures, so long as interfaces are connected correctly. There is no special component which the framework requires all systems to incorporate.

- @b Interfaces: individual components are free to provide or require any set of interfaces. There is no particular interface which all components must provide or require, and it is easy for component developers to invent new interfaces.

- @b Internals: component developers are free to provide the implementations of their components' interfaces (which are opaque to the rest of the system) in any way they choose.

- @b OS/Language: Orca was designed with the intention of being used on various platforms. To this end, we use <a href="http://www.cmake.org" target="_blank">CMake</a> (cross-platform make) for its build system.  Ice has language mappings for C++, Java, Python, C#, VisualBasic and PHP. It builds natively under various operating systems including Windows, Linux and Mac OS X. While the majority of components in the repository use Linux/C++ platform, nothing in principle should stop you from working with any one of the potential platform/language combinations. For GUI development we use <a href="http://www.trolltech.com" target="_blank">TrollTech's Qt</a> -- a multi-platform C++ GUI framework.

@image html xplatform_tools.jpg

So, the only thing we prescribe is the way in which interfaces are defined and implemented (using Ice), in order to enforce component compatibility.

@subsection orca_doc_about_tools Optional Tools

While we avoid enforcing particular design patterns for either systems
or components, we do provide guidelines (and working code!) for
designs that have worked well in the past.

For developing individual components, most of these established design
patterns are encoded in <a href="group__orca__library__orcaice.html">libOrcaIce</a>.
We find it useful -- if you do too then go ahead and use it.  But feel
free to copy it and modify it (or not use it at all) as required.


@subsection orca_doc_about_repository The Component Repository

We recognise that the usefulness of both the framework and any particular component increases with the variety and quality of the available components with which one can interact.  We therefore maintain @ref orca_doc_swmap "this online repository". Users are encouraged to contribute their own components and to point out omissions in components' documentation.

@section orca_doc_about_license License

The Orca project does not, as a policy, require code to conform to any specific 
license to be included in the distribution. However, the code is required to 
have a license and the license must be Open Source. That is the official policy.

The reality of the situation is that nearly all code in the Orca libraries and 
components is covered under the <a href="http://www.fsf.org/copyleft/lgpl.html" target="_blank">Library GNU Public License (LGPL)</a> as defined by 
the Free Software Foundation (FSF). Most of the remaining code is licensed under 
the <a href="http://www.fsf.org/copyleft/gpl.html" target="_blank">GNU Public License (GPL)</a>, also defined by the FSF.

The LGPL (as opposed to the GPL) is used whenever possible to allow commercial
enterprises to build closed-source products using Orca technology.
Note that Orca's only required dependency, Ice, is released under a <a href="http://www.zeroc.com/licensing.html" target="_blank">dual license</a>: it is GPL
unless a commercial license is purchased from <a href="http://www.zeroc.com" target="_blank">ZeroC</a>.

@section orca_doc_about_copyright Copyright

@section orca_doc_about_history History

Orca began as part of the EU-funded OROCOS Project: a project
to develop an Open-Source Robotic Control System.  The project
was a collaboration between four universities: <a href="http://www.cas.kth.se" target="_blank">KTH</a> (Sweden),
Leuven (Belgium), LAAS (France) and Ulm (Germany).  The four
institution worked on separate parts of a project that was
intended to eventually merge, but never did.  The Orocos name
now refers to <a href="http://www.orocos.org" target="_blank">the Belgian part</a>.

The task of <a href="http://www.cas.kth.se" target="_blank">KTH</a> was to build a set of communication patterns to enable the interaction of distributed components.  This work,
spearheaded by Anders Oreb&auml;ck, was called Orocos\@KTH.  By June 2004 Orocos\@KTH had also been adopted by members of the <a href="http://www.acfr.usyd.edu.au" target="_blank">Australian Centre for Field Robotics</a>.  By this time it had diverged far enough from the work at Leuven that a new name was required to disambiguate the two, and Orca1 was born. At that time the project moved to SourceForge. 

Work on porting Orca to Ice started in March of 2005. The new version called Orca2 was forked in late 2005, the main difference being the move to Ice middleware as the sole communications package. Switching to Ice has caused large changes in the component model. This means that Orca1 and Orca2 components are mutually incompatible.

At present, Orca is most actively used at several Universities in Sydney, Australia. For a sample of past and current projects see @ref orca_doc_projects.

@image html orca_sloc.jpg

Source code size gives some idea about the project's activity. The stats above are generated using David A. Wheeler's SLOCCount. This figure also helps illustrate our main objective: we are interested in a large "superstructure" (blue) -- useful components, and don't want to "dig" a deep "foundation" (orange) -- the infrastructure necessary to allow components to talk to each other. This was the main motivation for switching to Ice which is visible since v.2.0.0-rc1 at the end of 2005. Note that we count here only the C++ distribution of Ice. For comparison, we show a similar analysis of Player, up to distribution v.2.0.2. For the purposes of this figure, Player 2.0.2 "base" includes the contents of {libplayercore, client_libs, libplayertcp} and Player 2.0.2 "components" includes the code in {server, utils}. The autogenerated bindings are not taken into account.


*/
